{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loWkgpcdLlJW"
      },
      "source": [
        "# Describe the model: input-output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1WOK4xLo3T"
      },
      "source": [
        "**ใช้ Dataset English-French จาก kaggle**\n",
        "https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset <br/>\n",
        "\n",
        "\n",
        "**input** เป็น English-Text <br/>\n",
        "**output** เป็น French-Text<br/>\n",
        "ทำการแปลงภาษาจาก English -> French โดยใช้โมเดล **Bi-directional RNNs** และ Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NoyN6bvWIsjJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpIt1qMMtCi"
      },
      "source": [
        "# Show the code for running the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEwWaHojM2dn"
      },
      "source": [
        "**Import all library**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ก่อนหน้านี้ Run ใน collab แล้ว runtime มันเต็มผมเลยเอามารันใน VS code**"
      ],
      "metadata": {
        "id": "ma8ma03GIkw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA7HLT9quaey"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "import collections\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOp2coixxLyW"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tabulate import tabulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6YX89uPu3OJ",
        "outputId": "7e4f82de-a119-4bc5-bd53-cbfed4d32a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIRLviluM_1s"
      },
      "source": [
        "**นำข้อมูลมาใช้จากไดฟ์ **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "guC3o6L5u4vV",
        "outputId": "9ee3e48a-ee1f-490d-a83b-18c2437d76f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English words/sentences French words/sentences\n",
              "0                     Hi.                 Salut!\n",
              "1                    Run!                Cours !\n",
              "2                    Run!               Courez !\n",
              "3                    Who?                  Qui ?\n",
              "4                    Wow!             Ça alors !"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df =pd.read_csv('D:\\deep-learning\\RN\\eng_-french-50000.csv')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyI9-b4v2uD1"
      },
      "outputs": [],
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Eowr-JNztZn"
      },
      "outputs": [],
      "source": [
        "eng = df['English words/sentences']\n",
        "fr = df['French words/sentences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LOURydYzyjZ",
        "outputId": "dd0cb57b-9b16-489f-cd3c-e4fd1e507227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "192358 English words.\n",
            "9946 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"I\" \"a\" \"you\" \"is\" \"to\" \"the\" \"I'm\" \"He\" \"Tom\" \"was\"\n",
            "\n",
            "221631 French words.\n",
            "17389 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"Je\" \"?\" \"pas\" \"de\" \"Il\" \"!\" \"est\" \"ne\" \"le\" \"suis\"\n"
          ]
        }
      ],
      "source": [
        "#นับศัพท์\n",
        "english_words_counter = collections.Counter([word for sentence in eng for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in fr for word in sentence.split()])\n",
        "\n",
        "#แยกศัพท์ใช้บ่อย\n",
        "print('{} English words.'.format(len([word for sentence in eng for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in fr for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tO9c1ZX0c3E"
      },
      "outputs": [],
      "source": [
        "#ใช้tokenize\n",
        "def tokenize(x):\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7hVS8lX0lvC"
      },
      "outputs": [],
      "source": [
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ82AvHQ0mBx"
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n",
        "\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeyJBrlj0n90"
      },
      "outputs": [],
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(eng, fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvwPu_590pNP",
        "outputId": "b87ee0b3-69c5-4033-f9e5-f344ea8a9a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max English sentence length: 7\n",
            "Max French sentence length: 14\n",
            "English vocabulary size: 6004\n",
            "French vocabulary size: 12902\n"
          ]
        }
      ],
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9t8g7jx0svf"
      },
      "outputs": [],
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ' '\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybCtBC3aMB47"
      },
      "outputs": [],
      "source": [
        "#โมเดลแรกที่เอามาทดลอง\n",
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
        "    model.add(AttentionLayer())\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB6K-gBU01i0"
      },
      "source": [
        "**Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDSB6s4oMhMi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Implement the attention mechanism here\n",
        "        return inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "1sR5zYZD0z4J",
        "outputId": "eb8f76b0-4195-4f93-f6b9-93fe5164018d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 14, 256)           1537280   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 14, 512)           789504    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " attention_layer (Attention  (None, 14, 512)           0         \n",
            " Layer)                                                          \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 14, 1024)          525312    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 14, 12903)         13225575  \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16077671 (61.33 MB)\n",
            "Trainable params: 16077671 (61.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "625/625 [==============================] - 390s 613ms/step - loss: 1.9609 - accuracy: 0.7430 - val_loss: 2.0541 - val_accuracy: 0.7087\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 370s 592ms/step - loss: 1.4336 - accuracy: 0.7862 - val_loss: 1.8135 - val_accuracy: 0.7333\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 372s 595ms/step - loss: 1.1864 - accuracy: 0.8043 - val_loss: 1.6663 - val_accuracy: 0.7488\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 380s 608ms/step - loss: 1.0037 - accuracy: 0.8177 - val_loss: 1.5703 - val_accuracy: 0.7583\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 383s 613ms/step - loss: 0.8616 - accuracy: 0.8290 - val_loss: 1.5277 - val_accuracy: 0.7639\n"
          ]
        }
      ],
      "source": [
        "# Reshape the input\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "\n",
        "# Train\n",
        "model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=5, validation_split=0.2)\n",
        "#model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=5, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHn_zDuuV39a",
        "outputId": "32b2381d-d57d-42d2-916d-3397b17295d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"bd_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eIfiWsRIHUI",
        "outputId": "bb35c4dd-a0e6-4037-eaee-d694baa38774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------\n",
            "\n",
            "Original text:\n",
            "I dozed.\n",
            "\n",
            "-------------\n",
            "\n",
            "Prediction:\n",
            "1/1 [==============================] - 1s 888ms/step\n",
            "je me suis                      \n",
            "\n",
            "-------------\n",
            "\n",
            "Correct Translation:\n",
            "Je me suis assoupi.\n"
          ]
        }
      ],
      "source": [
        "i=150\n",
        "print(\"\\n-------------\")\n",
        "print(\"\\nOriginal text:\")\n",
        "print(eng[i])\n",
        "print(\"\\n-------------\")\n",
        "print(\"\\nPrediction:\")\n",
        "print(logits_to_text(model.predict(tmp_x[[i]])[0], french_tokenizer))\n",
        "print(\"\\n-------------\")\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(fr[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do3M0kmAPLUW",
        "outputId": "1ad49990-afcb-4943-a321-7e34bbdfeff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "+-------------------------+--------------------------------+----------------------------+\n",
            "| Original text           | Correct Translation            | Prediction                 |\n",
            "+=========================+================================+============================+\n",
            "| I'm often in trouble.   | Je suis souvent en difficulté. | je suis souvent de         |\n",
            "+-------------------------+--------------------------------+----------------------------+\n",
            "| They're here.           | Ils sont là.                   | ils sont là                |\n",
            "+-------------------------+--------------------------------+----------------------------+\n",
            "| It had started to rain. | Il avait commencé à pleuvoir.  | il m'a commencé à pleuvoir |\n",
            "+-------------------------+--------------------------------+----------------------------+\n",
            "| They'll let us know.    | Elles nous le feront savoir.   | ils nous nous              |\n",
            "+-------------------------+--------------------------------+----------------------------+\n",
            "| Do you feel guilty?     | Vous sentez-vous coupable ?    | vous vous tu coupable      |\n",
            "+-------------------------+--------------------------------+----------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Generate 5 random indices\n",
        "random_indices = random.sample(range(len(eng)), 5)\n",
        "\n",
        "# Create a list to store table rows\n",
        "table_data = []\n",
        "\n",
        "# Loop through the random indices and fill the table data\n",
        "for i in random_indices:\n",
        "    original_text = eng[i]\n",
        "    correct_translation = fr[i]\n",
        "    prediction = logits_to_text(model.predict(tmp_x[[i]])[0], french_tokenizer)\n",
        "\n",
        "    # Append data to the table\n",
        "    table_data.append([original_text, correct_translation, prediction])\n",
        "\n",
        "# Define column headers\n",
        "headers = [\"Original text\", \"Correct Translation\", \"Prediction\"]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YdjPnXsPRYx",
        "outputId": "7c8c560f-c0b5-488b-fa25-5fb6325cda36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "Predicted Translation:\n",
            "salut comment êtes tu                    \n"
          ]
        }
      ],
      "source": [
        "# ลองinput คำเอง\n",
        "input_text = \"Hello how are you\"\n",
        "\n",
        "# Tokenize and pad the input text\n",
        "input_sequence = pad_sequences(english_tokenizer.texts_to_sequences([input_text]), maxlen=preproc_french_sentences.shape[1], padding='post')\n",
        "# Make prediction\n",
        "predicted_logits = model.predict(input_sequence)\n",
        "\n",
        "# Convert logits to text using the tokenizer used for French sentences\n",
        "predicted_translation = logits_to_text(predicted_logits[0], french_tokenizer)\n",
        "\n",
        "# Print the predicted translation\n",
        "print(\"Predicted Translation:\")\n",
        "print(predicted_translation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPFcGCRNNsQi"
      },
      "source": [
        "# Show a custom dataset for fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCFUkHPnOKpC",
        "outputId": "708a8c7e-5b93-42c8-923f-3b3f712f106d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English words/sentences French words/sentences\n",
              "0                     Hi.                 Salut!\n",
              "1                    Run!                Cours !\n",
              "2                    Run!               Courez !\n",
              "3                    Who?                  Qui ?\n",
              "4                    Wow!             Ça alors !"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfTune =pd.read_csv('D:\\deep-learning\\RN\\eng_-french-Tune.csv')\n",
        "dfTune.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zai7lnsBOgIl"
      },
      "outputs": [],
      "source": [
        "eng = dfTune['English words/sentences']\n",
        "fr = dfTune['French words/sentences']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sXBKD2xOi8P",
        "outputId": "22ec7065-e6c2-4b58-cc1c-a61de1f06d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "464941 English words.\n",
            "15788 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"I\" \"you\" \"to\" \"a\" \"the\" \"is\" \"Tom\" \"He\" \"I'm\" \"was\"\n",
            "\n",
            "520023 French words.\n",
            "27315 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"Je\" \"?\" \"pas\" \"de\" \"que\" \"ne\" \"Il\" \"à\" \"le\" \"Tom\"\n"
          ]
        }
      ],
      "source": [
        "#นับศัพท์\n",
        "english_words_counter = collections.Counter([word for sentence in eng for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in fr for word in sentence.split()])\n",
        "\n",
        "#แยกศัพท์ใช้บ่อย\n",
        "print('{} English words.'.format(len([word for sentence in eng for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in fr for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lza7WOQyOk8b"
      },
      "outputs": [],
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(eng, fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3COKgJM2Omt0",
        "outputId": "e4483eeb-6c46-4a56-aebf-f3237a8461e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max English sentence length: 9\n",
            "Max French sentence length: 15\n",
            "English vocabulary size: 8907\n",
            "French vocabulary size: 19582\n"
          ]
        }
      ],
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGB-fmCBJGW3"
      },
      "source": [
        "# Show the code for fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "715doD7KqA8v"
      },
      "outputs": [],
      "source": [
        "def bd_model_Tune(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 512, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(Bidirectional(GRU(512, return_sequences=True)))\n",
        "    #model.add(Attention())  # Add Attention layer, configure based on your specific use case\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZfG7KYzWDdg",
        "outputId": "c4b68f3a-804b-4613-94e1-3cba26f1d7d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 15, 512)           4560896   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 15, 1024)          3151872   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, 15, 1024)          1049600   \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 15, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDi  (None, 15, 19583)         20072575  \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28834943 (110.00 MB)\n",
            "Trainable params: 28834943 (110.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1094/1094 [==============================] - 1240s 1s/step - loss: 1.9106 - accuracy: 0.7418 - val_loss: 2.1554 - val_accuracy: 0.6884\n",
            "Epoch 2/5\n",
            "1094/1094 [==============================] - 2402s 2s/step - loss: 1.3491 - accuracy: 0.7894 - val_loss: 1.8461 - val_accuracy: 0.7172\n",
            "Epoch 3/5\n",
            "1094/1094 [==============================] - 1158s 1s/step - loss: 1.0828 - accuracy: 0.8094 - val_loss: 1.7003 - val_accuracy: 0.7299\n",
            "Epoch 4/5\n",
            "1094/1094 [==============================] - 1214s 1s/step - loss: 0.9091 - accuracy: 0.8234 - val_loss: 1.6366 - val_accuracy: 0.7358\n",
            "Epoch 5/5\n",
            "1094/1094 [==============================] - 1219s 1s/step - loss: 0.7867 - accuracy: 0.8346 - val_loss: 1.6330 - val_accuracy: 0.7407\n"
          ]
        }
      ],
      "source": [
        "# Reshape the input\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "\n",
        "# Train\n",
        "model = bd_model_Tune(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=5, validation_split=0.3)\n",
        "#model.fit(tmp_x, preproc_french_sentences, batch_size=64, epochs=5, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUL-afbBWAdo",
        "outputId": "132ac607-9f88-4bab-d1b4-6a6ab5340936"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"bd_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e1h_ExurglS",
        "outputId": "0596f81d-f556-4712-a2bf-cd712e805d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "1/1 [==============================] - 1s 907ms/step\n",
            "bon boulot                          \n",
            "\n",
            "Correct Translation:\n",
            "Bien joué !\n",
            "\n",
            "Original text:\n",
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "i=275\n",
        "\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(logits_to_text(model.predict(tmp_x[[i]])[0], french_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(fr[i])\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(eng[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB7G3bGQJONQ",
        "outputId": "581265de-8a71-4d41-8ef5-d923051ae9c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "+-------------------------------+-----------------------------------+---------------------------+\n",
            "| Original text                 | Correct Translation               | Prediction                |\n",
            "+===============================+===================================+===========================+\n",
            "| We must carry out that plan.  | Il faut exécuter ce plan.         | nous nous faut ce ce      |\n",
            "+-------------------------------+-----------------------------------+---------------------------+\n",
            "| He kept his hat on.           | Il garda son chapeau sur la tête. | il a son son chapeau      |\n",
            "+-------------------------------+-----------------------------------+---------------------------+\n",
            "| That could've been prevented. | Cela aurait pu être évité.        | ça aurait pu être         |\n",
            "+-------------------------------+-----------------------------------+---------------------------+\n",
            "| What a beautiful scene!       | Quel beau tableau !               | quel belle                |\n",
            "+-------------------------------+-----------------------------------+---------------------------+\n",
            "| It's still in good condition. | C'est encore en bon état.         | c'est encore encore bonne |\n",
            "+-------------------------------+-----------------------------------+---------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Generate 5 random indices\n",
        "random_indices = random.sample(range(len(eng)), 5)\n",
        "\n",
        "# Create a list to store table rows\n",
        "table_data = []\n",
        "\n",
        "# Loop through the random indices and fill the table data\n",
        "for i in random_indices:\n",
        "    original_text = eng[i]\n",
        "    correct_translation = fr[i]\n",
        "    prediction = logits_to_text(model.predict(tmp_x[[i]])[0], french_tokenizer)\n",
        "\n",
        "    # Append data to the table\n",
        "    table_data.append([original_text, correct_translation, prediction])\n",
        "\n",
        "# Define column headers\n",
        "headers = [\"Original text\", \"Correct Translation\", \"Prediction\"]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f38UhpbRINqP",
        "outputId": "208a4660-6701-4057-b677-8f813e802ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "Predicted Translation:\n",
            "bonjour comment vous vous                      \n"
          ]
        }
      ],
      "source": [
        "# ลองinput คำเอง\n",
        "input_text = \"Hello how are you\"\n",
        "\n",
        "# Tokenize and pad the input text\n",
        "input_sequence = pad_sequences(english_tokenizer.texts_to_sequences([input_text]), maxlen=preproc_french_sentences.shape[1], padding='post')\n",
        "# Make prediction\n",
        "predicted_logits = model.predict(input_sequence)\n",
        "\n",
        "# Convert logits to text using the tokenizer used for French sentences\n",
        "predicted_translation = logits_to_text(predicted_logits[0], french_tokenizer)\n",
        "\n",
        "# Print the predicted translation\n",
        "print(\"Predicted Translation:\")\n",
        "print(predicted_translation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare the performance before and after fine-tuning"
      ],
      "metadata": {
        "id": "-20JwZixFVoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**หลังจากทดสอบ** <br/>\n",
        "**Model ก่อน Fine-Tuning** พบว่า มี Acuuracy ที่ดีที่สุดอยู่ที่ accuracy: 0.8290 <br/>\n",
        "**และหลังจากทำการ Fine-Tuning** accuracy: 0.8346 <br/>\n",
        "โมเดลสามารถใช้งาน predict ภาษา French จากการ input ภาษาอังกฤษได้แต่อาจมีความคลาดเคลื่อนบ้าง"
      ],
      "metadata": {
        "id": "HZrTuregFlLa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9j46r7ZneM0"
      },
      "source": [
        "# Build a model from scratch, train the model with the custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ทำการเลือก Dataset มาใช้ และทำการ Trainmodel ใช้ seq2seq"
      ],
      "metadata": {
        "id": "ginwx7wJH0Dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJSaYB6Fd2Fy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "df =pd.read_csv('D:\\deep-learning\\RN\\eng_-french-50000.csv')\n",
        "english_sentences = df[\"English words/sentences\"].tolist()\n",
        "french_sentences = df[\"French words/sentences\"].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqgkbk3U96_F"
      },
      "outputs": [],
      "source": [
        "# Adaptation des tokenizers aux données\n",
        "tokenizer_eng = Tokenizer()\n",
        "tokenizer_eng.fit_on_texts(english_sentences)\n",
        "eng_seq = tokenizer_eng.texts_to_sequences(english_sentences)\n",
        "\n",
        "tokenizer_fr = Tokenizer()\n",
        "tokenizer_fr.fit_on_texts(french_sentences)\n",
        "fr_seq = tokenizer_fr.texts_to_sequences(french_sentences)\n",
        "\n",
        "\n",
        "vocab_size_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_size_fr = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "# Padding\n",
        "max_length = max(len(seq) for seq in eng_seq + fr_seq)\n",
        "eng_seq_padded = pad_sequences(eng_seq, maxlen=max_length, padding='post')\n",
        "fr_seq_padded = pad_sequences(fr_seq, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzeogorZ98jQ",
        "outputId": "206cc27f-1bd7-46c9-c135-75b593e1ab69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_length,))\n",
        "enc_emb = Embedding(input_dim=vocab_size_eng, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_length,))\n",
        "dec_emb_layer = Embedding(input_dim=vocab_size_fr, output_dim=embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_fr, activation='softmax')\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Modèle\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpTDe5VC-NS6",
        "outputId": "0dec6077-ab12-4ed1-cd45-ca9aa3a6f4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 320s 503ms/step - loss: 2.0895 - accuracy: 0.7226 - val_loss: 1.7563 - val_accuracy: 0.7520\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 321s 513ms/step - loss: 1.5848 - accuracy: 0.7640 - val_loss: 1.5218 - val_accuracy: 0.7733\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 310s 495ms/step - loss: 1.3352 - accuracy: 0.7819 - val_loss: 1.3790 - val_accuracy: 0.7846\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 317s 507ms/step - loss: 1.1422 - accuracy: 0.7944 - val_loss: 1.2801 - val_accuracy: 0.7946\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 294s 471ms/step - loss: 0.9807 - accuracy: 0.8073 - val_loss: 1.2048 - val_accuracy: 0.8001\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x228d7186810>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(eng_seq_padded, fr_seq_padded, test_size=0.2)\n",
        "model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3a44wyW-QAz",
        "outputId": "1fcdfe22-931f-4832-b113-552d2fc736d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"my_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cn6TrM_-S84"
      },
      "outputs": [],
      "source": [
        "#model = tf.keras.models.load_model(\"seq2seq_translation_v3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G52TokNA-Vct",
        "outputId": "38a86813-8c20-452a-bb95-233889bdc5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Input: Hi! I am tired .\n",
            "Translated: salut je suis                      \n"
          ]
        }
      ],
      "source": [
        "def translate_sentence(sentence):\n",
        "    seq = tokenizer_eng.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "    translated = np.argmax(model.predict([padded, padded]), axis=-1)\n",
        "\n",
        "    translated_sentence = []\n",
        "    for i in translated[0]:\n",
        "        if i in tokenizer_fr.index_word:\n",
        "            translated_sentence.append(tokenizer_fr.index_word[i])\n",
        "        else:\n",
        "            translated_sentence.append(' ')\n",
        "\n",
        "    return ' '.join(translated_sentence)\n",
        "\n",
        "input_sentence = \"Hi! I am tired .\"\n",
        "translated_sentence = translate_sentence(input_sentence)\n",
        "print(f\"Input: {input_sentence}\")\n",
        "print(f\"Translated: {translated_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qxgl6DsQ9nO",
        "outputId": "66897e59-d01e-4d65-8c76-4f5c62d57acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "+-------------------------------+-------------------------------------+----------------------+\n",
            "| Original Text                 | Correct Translation                 | Prediction           |\n",
            "+===============================+=====================================+======================+\n",
            "| Don't stay out all night.     | Ne reste pas dehors toute la nuit ! | ne ne pas pas la la  |\n",
            "+-------------------------------+-------------------------------------+----------------------+\n",
            "| I get off there, too.         | Je descends ici aussi.              | je vous à là         |\n",
            "+-------------------------------+-------------------------------------+----------------------+\n",
            "| Now is your chance.           | C'est maintenant ta chance.         | maintenant est votre |\n",
            "+-------------------------------+-------------------------------------+----------------------+\n",
            "| I've already read that novel. | J'ai déjà lu ce roman.              | je déjà déjà ça ça   |\n",
            "+-------------------------------+-------------------------------------+----------------------+\n",
            "| He seems to think so.         | Il semble penser cela.              | il a être à être     |\n",
            "+-------------------------------+-------------------------------------+----------------------+\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def translate_sentence(sentence):\n",
        "    seq = tokenizer_eng.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding=\"post\")\n",
        "    translated = np.argmax(model.predict([padded, padded]), axis=-1)\n",
        "\n",
        "    translated_sentence = []\n",
        "    for i in translated[0]:\n",
        "        if i in tokenizer_fr.index_word:\n",
        "            translated_sentence.append(tokenizer_fr.index_word[i])\n",
        "        else:\n",
        "            translated_sentence.append(\" \")\n",
        "\n",
        "    return \" \".join(translated_sentence)\n",
        "\n",
        "# Generate 5 random indices\n",
        "random_indices = random.sample(range(len(eng)), 5)\n",
        "\n",
        "# Create a list to store table rows\n",
        "table_data = []\n",
        "\n",
        "# Define column headers\n",
        "headers = [\"Original Text\", \"Correct Translation\", \"Prediction\"]\n",
        "\n",
        "# Loop through the random indices and fill the table data\n",
        "for i in random_indices:\n",
        "    # Retrieve data\n",
        "    original_text = eng[i]\n",
        "    correct_translation = fr[i]\n",
        "    prediction = translate_sentence(original_text)\n",
        "\n",
        "    # Append data to the table\n",
        "    table_data.append([original_text, correct_translation, prediction])\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "dWjvsTJ_IyRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "หลังจากการทดลองใช้ Model และ สร้างโมเดล<br/>\n",
        "พบว่า Model ที่มีมาให้ **ให้ค่า Accuracy ดีที่สุดอยู่ที่ 0.8346**\n",
        "โดยหากเพิ่มจำนวณข้อมูลในการ train ให้มากกว่านี้ค่า Accuracy ที่ได้ก็จะมากขึ้นตามไปด้วย<br/>\n",
        "ส่วน Model ที่สร้างขึ้นมาเองดีใช้ได้เหมือนกันแต่ไม่ดีเท่า Model ที่มีให้<br/>\n"
      ],
      "metadata": {
        "id": "NR9mJz7tKAmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**นั่งทำนานมาครับ สามารถ predict จาก Input ได้ด้วย ขอคะแนนพิศวาสหน่อยครับ T T**"
      ],
      "metadata": {
        "id": "Kk8pJIu1LbUh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}